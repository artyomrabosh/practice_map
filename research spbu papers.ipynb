{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6fa6f0",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import sentenize\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d46a9d",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f26d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/se_spbu/works.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48106b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_list_to_list(string):\n",
    "    return string[2:-1].split(\"', '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de83c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text):\n",
    "    text = string_list_to_list(text)\n",
    "    text = \" \".join(text[1:]) # введение\n",
    "    text = re.sub(\"- \", \"\", text) # переносы строки\n",
    "    \n",
    "    formulas_characters = \"@#^&*+_=<✓α>/≡≡Σ∈≤\"\n",
    "    chars = '●•'\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', chars))\n",
    "    text = list(sentenize(text))\n",
    "    \n",
    "    \n",
    "    sents = [sent.text for sent in text]\n",
    "    sents = list(filter(lambda x: \"аблица\" not in x, sents))\n",
    "    sents = list(filter(lambda x: \"траница\" not in x, sents))\n",
    "    sents = list(filter(lambda x: \"исунок\" not in x, sents))\n",
    "    sents = list(filter(lambda x: len(x)>10, sents))\n",
    "    sents = list(filter(lambda x: len(x)<1500, sents))\n",
    "    sents = list(filter(lambda x: not any(c in formulas_characters for c in x), sents))\n",
    "    \n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_texts\"] = df.texts.apply(preprocessing_pipeline)\n",
    "df['preprocessed_len'] = df.preprocessed_texts.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# НАЗВАНИЕ СЕКЦИЙ\n",
    "# РИСУНОК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9276380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['preprocessed_len'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bf38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df[:-10]\n",
    "val_texts = df[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec692b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df.preprocessed_texts.explode(ignore_index=True)\n",
    "train_sentences = train_texts.preprocessed_texts.explode(ignore_index=True)\n",
    "val_sentences = val_texts.preprocessed_texts.explode(ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d70a29",
   "metadata": {},
   "source": [
    "# sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'blanchefort/rubert-base-cased-sentiment-rusentiment'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict ={\n",
    "    0: \"neutral\",\n",
    "    1: \"positive\",\n",
    "    2: \"negative\"\n",
    "}\n",
    "def get_sentiment(text, return_type='label'):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
    "        proba = model(**inputs).logits.cpu().numpy()[0]\n",
    "    if return_type == 'label':\n",
    "        return sent_dict[proba.argmax()]\n",
    "        return model.config.id2label[proba.argmax()]\n",
    "    elif return_type == 'score':\n",
    "        return proba.dot([-1, 0, 1])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Какая гадость эта ваша заливная рыба!'\n",
    "print(get_sentiment(text, 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23527e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for sentence in df_sentences:\n",
    "    sent = get_sentiment(sentence)\n",
    "    if sent != \"neutral\":\n",
    "        print(sent + \": \" + sentence)\n",
    "        num += 1\n",
    "print(\"Всего: \", num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c790f",
   "metadata": {},
   "source": [
    "## морфология"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed70a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_sentences.apply(Doc)\n",
    "docs.progress_apply(lambda x: x.segment(segmenter))\n",
    "docs.progress_apply(lambda x: x.tag_morph(morph_tagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_with_morph = []\n",
    "for doc in docs:\n",
    "    try:\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        print(doc)\n",
    "        docs_with_morph.append(doc)\n",
    "    except:\n",
    "        docs_with_morph.append(None)\n",
    "\n",
    "docs = pd.Series(docs_with_morph)\n",
    "# docs = docs.apply(lambda x: x.tag_morph(morph_tagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for sent in docs:\n",
    "    #if \" Я \" in sent.text or \" я \" in sent.text:\n",
    "    #        num+=1\n",
    "            #print(\"From text:\", sent.text)\n",
    "            #for token in sent.morph.tokens:\n",
    "            #    pass\n",
    "            #    print(token)\n",
    "            \n",
    "    for token in sent.morph.tokens:\n",
    "        #print(token)\n",
    "        #print(sent)\n",
    "        if token.pos == \"VERB\":\n",
    "            if \"Number\" in token.feats and \"Person\" in token.feats:\n",
    "                if token.feats['Person'] == \"1\" and token.feats[\"Number\"] == \"Sing\":\n",
    "                    pass\n",
    "                    num+=1\n",
    "                    print(\"token : \", token.text)\n",
    "                    print(\"From text:\", sent.text)\n",
    "print(\"Всего :\", num)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6057b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
