{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec662d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3489901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "import contextlib\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, logs_path: tp.Union[str, os.PathLike]):\n",
    "        self.path = pathlib.Path(logs_path)\n",
    "\n",
    "        records = []\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.json'):\n",
    "                    uuid = os.path.splitext(file)[0]\n",
    "                    with open(os.path.join(root, file), 'r') as f:\n",
    "                        try:\n",
    "                            logged_data = json.load(f)\n",
    "                            records.append(\n",
    "                                {\n",
    "                                    'id': uuid,\n",
    "                                    **logged_data\n",
    "                                }\n",
    "                            )\n",
    "                        except json.JSONDecodeError:\n",
    "                            pass\n",
    "        if records:\n",
    "            self.leaderboard = pd.DataFrame.from_records(records, index='id')\n",
    "        else:\n",
    "            self.leaderboard = pd.DataFrame(index=pd.Index([], name='id'))\n",
    "\n",
    "        self._current_run = None\n",
    "\n",
    "    class Run:\n",
    "\n",
    "        def __init__(self, name, storage, path):\n",
    "            self.name = name\n",
    "            self._storage = storage\n",
    "            self._path = path\n",
    "            self._storage.append(pd.Series(name=name))\n",
    "\n",
    "        def log(self, key, value):\n",
    "            self._storage.loc[self.name, key] = value\n",
    "\n",
    "        def log_values(self, log_values: tp.Dict[str, tp.Any]):\n",
    "            for key, value in log_values.items():\n",
    "                self.log(key, value)\n",
    "\n",
    "        def save_logs(self):\n",
    "            with open(self._path / f'{self.name}.json', 'w+') as f:\n",
    "                json.dump(self._storage.loc[self.name].to_dict(), f)\n",
    "\n",
    "        def log_artifact(self, fname: str, writer: tp.Callable):\n",
    "            with open(self._path / fname, 'wb+') as f:\n",
    "                writer(f)\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def run(self, name: tp.Optional[str] = None):\n",
    "        if name is None:\n",
    "            name = str(uuid.uuid4())\n",
    "        elif name in self.leaderboard.index:\n",
    "            raise NameError(\"Run with given name already exists, name should be unique\")\n",
    "        else:\n",
    "            name = name.replace(' ', '_')\n",
    "        self._current_run = Logger.Run(name, self.leaderboard, self.path / name)\n",
    "        os.makedirs(self.path / name, exist_ok=True)\n",
    "        try:\n",
    "            yield self._current_run\n",
    "        finally:\n",
    "            self._current_run.save_logs()\n",
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b760647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cointegrated/rubert-tiny2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f859aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificationDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, MAX_LEN):\n",
    "        # вбрасываю сюда всю предобработку\n",
    "        super(classificationDataset).__init__()\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.labels =self.df.new_target\n",
    "        self.token = self.df.text.progress_apply(lambda x: tokenizer.encode_plus(str(x), \n",
    "                        add_special_tokens=True,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        max_length=MAX_LEN,\n",
    "                        return_attention_mask=True,   \n",
    "                        return_tensors='pt'    \n",
    "                   ))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = torch.squeeze(self.token[index]['input_ids'])\n",
    "        token_type_ids = torch.squeeze(self.token[index]['token_type_ids'])\n",
    "        attention_mask = torch.squeeze(self.token[index]['attention_mask'])\n",
    "        return input_ids, token_type_ids, attention_mask, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3ae2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = classificationDataset(\"labeled_train.csv\", tokenizer, 128)\n",
    "val = classificationDataset(\"labeled_val.csv\", tokenizer, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d667575",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset=val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49d8b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,   \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(dataloader_train) * epochs\n",
    "\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(                \n",
    "                optimizer = optimizer,\n",
    "                num_warmup_steps = 0,\n",
    "                num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "# Running the model on GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def flat_accuracy(logits, labels):\n",
    "    return (logits == labels).sum() / len(labels)\n",
    "\n",
    "def flat_recall(logits, labels):\n",
    "    pred_flat = logits.argmax(axis=1)\n",
    "    return Recall('binary', pred_flat, labels)\n",
    "\n",
    "def flat_precision(logits, labels):\n",
    "    pred_flat = logits.argmax(axis=1)\n",
    "    return Precision(pred_flat, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train(model, opt, loader, scheduler):\n",
    "    model.train()\n",
    "    losses_tr = []\n",
    "    for input_ids, attention_masks, token_type_ids, labels in tqdm(loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, \n",
    "                    token_type_ids=token_type_ids, \n",
    "                    attention_mask=attention_masks, \n",
    "                    labels=labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        losses_tr.append(loss.item())\n",
    "    \n",
    "    return model, optimizer, np.mean(losses_tr)\n",
    "\n",
    "\n",
    "def val(model, loader, metric_names=None):\n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    if metric_names is not None:\n",
    "        metrics = defaultdict(list)\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_masks, token_type_ids, labels in tqdm(loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, \n",
    "                    token_type_ids=token_type_ids, \n",
    "                    attention_mask=attention_masks, \n",
    "                    labels=labels)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "            losses_val.append(outputs.loss.item())\n",
    "\n",
    "            if metric_names is not None:\n",
    "                pred_flat = outputs.logits.argmax(axis=1)\n",
    "                pred_flat.to(device)\n",
    "                if 'accuracy' in metric_names:\n",
    "                    accuracy = Accuracy(task=\"binary\")\n",
    "                    accuracy.to(device)\n",
    "                    metrics[\"accuracy\"].append(flat_accuracy(pred_flat, labels).item())\n",
    "                if 'precision' in metric_names:\n",
    "                    precision = Precision(task=\"binary\")\n",
    "                    precision.to(device)\n",
    "                    metrics[\"precision\"].append(precision(pred_flat, labels).item())\n",
    "                if 'recall' in metric_names:\n",
    "                    recall = Recall(task=\"binary\")\n",
    "                    recall.to(device)\n",
    "                    metrics[\"recall\"].append(recall(pred_flat, labels).item())\n",
    "\n",
    "\n",
    "        if metric_names is not None:\n",
    "            for name in metrics:\n",
    "                metrics[name] = np.mean(metrics[name])\n",
    "    \n",
    "    return np.mean(losses_val), metrics if metric_names else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd05a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Accuracy\": Accuracy(\"binary\"),\n",
    "    \"Precision\": Precision(\"binary\"),\n",
    "    \"Recall\": Recall(\"binary\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = {\n",
    "        \"accuracy\": {\"plot_id\": 1},\n",
    "        \"precision\": {\"plot_id\": 2},\n",
    "        \"recall\": {\"plot_id\": 3}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def learning_loop(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    scheduler=None,\n",
    "    min_lr=None,\n",
    "    epochs=10,\n",
    "    val_every=1,\n",
    "    draw_every=1,\n",
    "    separate_show=False,\n",
    "    model_name=None,\n",
    "    metrics_names=None,\n",
    "):\n",
    "     \n",
    "    losses = {'train': [], 'val': []}\n",
    "    lrs = []\n",
    "    best_val_loss = np.Inf\n",
    "    if metrics_names is not None:\n",
    "        metrics = defaultdict(list)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        if epoch == 2:\n",
    "            for param in model.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        \n",
    "        model, optimizer, loss = train(model, optimizer, train_loader, scheduler)\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        if not (epoch % val_every):\n",
    "            loss, metrics_ = val(model, val_loader, metric_names=metrics_names)\n",
    "            losses['val'].append(loss)\n",
    "            if metrics_ is not None:\n",
    "                for name, value in metrics_.items():\n",
    "                    metrics[name].append(value)\n",
    "            \n",
    "            if scheduler:\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            ww = 3 if separate_show else 2\n",
    "            ww_metrics = 0\n",
    "            if metrics_names is not None:\n",
    "                plot_ids_ = [\n",
    "                    [key, metric_meta.get(\"plot id\", 1)]\n",
    "                    for key, metric_meta\n",
    "                    in metrics_names.items()\n",
    "                ]\n",
    "                ww_metrics = len(set(el[1] for el in plot_ids_))\n",
    "                assert all(el[1] <= ww_metrics for el in plot_ids_)\n",
    "                \n",
    "                plot_ids = defaultdict(list)\n",
    "                for el in plot_ids_:\n",
    "                    plot_ids[el[1]].append(el[0])\n",
    "                \n",
    "            fig, ax = plt.subplots(1, ww + ww_metrics, figsize=(20, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs}:')\n",
    "\n",
    "            plt.subplot(1, ww + ww_metrics, 1)\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            if separate_show:\n",
    "                plt.title('loss on train')\n",
    "                plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            if separate_show:\n",
    "                plt.subplot(1, ww + ww_metrics, 2)\n",
    "                plt.title('loss on validation')\n",
    "                plt.grid()\n",
    "            else:\n",
    "                plt.title('losses')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, ww + ww_metrics, ww)\n",
    "            plt.title('learning rate')\n",
    "            plt.plot(lrs, 'g.-', label='lr')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.grid()\n",
    "            \n",
    "            if metrics_names is not None:\n",
    "                for plot_id, keys in plot_ids.items():\n",
    "                    for key in keys:\n",
    "                        plt.subplot(1, ww + ww_metrics, ww + plot_id)\n",
    "                        plt.title(f'additional metrics #{plot_id}')\n",
    "                        for name in metrics:\n",
    "                            if key in name:\n",
    "                                plt.plot(metrics[name], '.-', label=name)\n",
    "                        plt.legend()\n",
    "                        plt.grid()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b7ab5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_name = model.name_or_path.replace(\"/\", \"-\") + f\"-{len(dataloader_train)}\" \n",
    "\n",
    "model, optimizer, losses = learning_loop(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = dataloader_train,\n",
    "    val_loader = dataloader_val,\n",
    "    scheduler = scheduler,\n",
    "    epochs = epochs,\n",
    "    min_lr = 1e-7,\n",
    "    val_every = 1,\n",
    "    draw_every = 1,\n",
    "    separate_show = False,\n",
    "    metrics_names = metrics_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe12cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"small_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = model.name_or_path.replace(\"/\", \"-\") + f\"-{len(dataloader_train)}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with logger.run(name=run_name) as run:\n",
    "            run.log_values(result_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
